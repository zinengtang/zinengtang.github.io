<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Zineng Tang</title>
  
  <meta name="author" content="Zineng Tang">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Zineng Tang</name>
              </p>
              <p> I am a fourth year undergraduate student majoring in Mathematics at the University of North Carolina at Chapel Hill.
		  I am advised by <a href="https://www.cs.unc.edu/~mbansal/">Mohit Bansal</a> in <a href="https://nlp.cs.unc.edu/">UNC-NLP</a>, <a href="https://murgelab.cs.unc.edu/">MURGe-Lab</a>. 
	          I have interned at Microsoft Research (in 2022) supervised by <a href="https://scholar.google.com/citations?user=JkyLIM0AAAAJ&hl=en">Ziyi Yang</a>.
		  I am honored to be selected as winner of the <a href="https://cra.org/2023-outstanding-undergraduate-researcher-award-recipients/">2023 CRA Outstanding Undergraduate Researcher Award</a>.
              </p>
              <p style="text-align:center">
                <a href="mailto:zn.tang.terran@gmail.com">Email</a> &nbsp/&nbsp
                <a href="images/CV.pdf">CV</a> &nbsp/&nbsp
                <!-- <a href="data/JonBarron-bio.txt">Bio</a> &nbsp/&nbsp -->
                <a href="https://scholar.google.com/citations?user=bZy4vtwAAAAJ&hl=zh-CN">Google Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/ZinengTang">Twitter</a> &nbsp/&nbsp
                <a href="https://github.com/zinengtang">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:30%;max-width:30%">
              <a href="images/zineng.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/zineng.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                My primary research interests lie in the area of multi-modal learning, natural language processing, and machine learning.
              </p>
            </td>
          </tr>
        </tbody></table>
		
		<script type="text/javascript">
		  function start() {
			document.getElementById('tvlt_image').style.opacity = "1";
		  }
		  function stop() {
			document.getElementById('tvlt_image').style.opacity = "0";
		  }
		  stop()
		</script>
		
        <table style="width:100%;border:0px;border-collapse:separate;border-spacing:0px;margin-right:auto;margin-left:auto;"><tbody>
		
			  <tr onmouseout="stop()" onmouseover="start()">
			  <td style="padding:40px;width:35%;vertical-align:middle;padding-top:80px">
				<div class="one">
				  <img src='images/udop.png' width="250" style=>
				</div>
				<!-- </td> -->
				
			  </td>
			  <td style="padding:40px;width:75%;vertical-align:middle;padding-top:80px">
				<a href="https://arxiv.org/pdf/2212.02623">
				  <papertitle>Unifying Vision, Text, and Layout for Universal Document Processing</papertitle>
				</a>
				<br>
				<strong>Zineng Tang*</strong>,
				<a href="https://www.microsoft.com/en-us/research/people/ziyiyang/">Ziyi Yang</a>,
				<a href="https://www.guoxwang.com/">Guoxin Wang</a>,
				<a href="https://www.microsoft.com/en-us/research/people/yuwfan/">Yuwei Fang</a>,
				<a href="https://nlp-yang.github.io/">Yang Liu</a>,
				<a href="https://cs.stanford.edu/people/cgzhu/">Chenguang Zhu</a>,
				<a href="https://www.microsoft.com/en-us/research/people/nzeng/">Michael Zeng</a>,
				<a href="https://www.microsoft.com/en-us/research/people/chazhang/">Cha Zhang</a>,
			        <a href="https://www.cs.unc.edu/~mbansal/">Mohit Bansal</a>
				<br>
				<em>Arxiv</em>
				<br>
				<a href="https://github.com/microsoft/UDOP">github</a>
				/
				<a href="https://arxiv.org/pdf/2212.02623">arXiv</a>
				<p></p>
				<p>
				We built a unified framework for document processing.
				</p>
			  </td>
			</tr>
					
          	          <tr onmouseout="stop()" onmouseover="start()">
			  <td style="padding:40px;width:35%;vertical-align:middle;padding-top:80px">
				<div class="one">
				  <img src='images/tvlt.png' width="250" style=>
				</div>
				<!-- </td> -->
				
			  </td>
			  <td style="padding:40px;width:75%;vertical-align:middle;padding-top:80px">
				<a href="https://arxiv.org/abs/2209.14156">
				  <papertitle>TVLT: Textless Vision-Language Transformer</papertitle>
				</a>
				<br>
				<strong>Zineng Tang*</strong>,
				<a href="https://j-min.io/">Jaemin Cho</a>,
				<a href="https://easonnie.github.io/">Yixin Nie</a>,
			  <a href="https://www.cs.unc.edu/~mbansal/">Mohit Bansal</a>
				<br>
				<em>NeurIPS</em>, 2022 <span style="color: red">(selected as Oral; 1.76% acceptance rate)</span>
				<br>
				<a href="https://github.com/zinengtang/TVLT">github</a>
				/
				<a href="https://arxiv.org/abs/2209.14156">arXiv</a>
				<p></p>
				<p>
				We built a textless vision-language transformer with a minimalist design.
				</p>
			  </td>
			</tr>
		  
			<tr onmouseout="stop()" onmouseover="start()">
			  <td style="padding:40px;width:35%;vertical-align:middle;padding-top:80px">
				<div class="one">
				  <img src='images/vidlankd.png' width="250">
				</div>
			  </td>
			  <td style="padding:20px;width:75%;vertical-align:middle">
				<a href="https://arxiv.org/pdf/2107.02681">
				  <papertitle>Vidlankd: Improving language understanding via video-distilled knowledge transfer</papertitle>
				</a>
				<br>
				<strong>Zineng Tang</strong>,
				<a href="https://j-min.io/">Jaemin Cho</a>,
				<a href="https://scholar.google.com/citations?user=OV1Y3FUAAAAJ&hl=en">Hao Tan</a>,
			  <a href="https://www.cs.unc.edu/~mbansal/">Mohit Bansal</a>
				<br>
				<em>NeurIPS</em>, 2021
				<br>
				<a href="https://github.com/zinengtang/VidLanKD">github</a>
				/
				<a href="https://arxiv.org/pdf/2107.02681">arXiv</a>
				<p></p>
				<p>
				We built a teacher-student transformer for visually grounded language learning.
				</p>
			  </td>
			</tr>
			
			<tr onmouseout="stop()" onmouseover="start()">
			  <td style="padding:40px;width:35%;vertical-align:middle;padding-top:80px">
				<div class="one">
				  <img src='images/decembert.png' width="250">
				</div>
			  </td>
			  <td style="padding:20px;width:75%;vertical-align:middle">
				<a href="https://aclanthology.org/2021.naacl-main.193/">
				  <papertitle>Decembert: Learning from noisy instructional videos via dense captions and entropy minimization</papertitle>
				</a>
				<br>
				<strong>Zineng Tang*</strong>,
				<a href="https://jayleicn.github.io/">Jie Lei*</a>,
			  <a href="https://www.cs.unc.edu/~mbansal/">Mohit Bansal</a>
				<br>
				<em>NAACL</em>, 2021
				<br>
				<a href="https://github.com/zinengtang/DeCEMBERT">github</a>
				/
				<a href="https://aclanthology.org/2021.naacl-main.193/">paper</a>
				<p></p>
				<p>
				We built a video-language transformer that addresses the issues of noisy ASR data by dense captions and entropy minimization.
				</p>
			  </td>
			</tr>		
			<tr onmouseout="stop()" onmouseover="start()">
			  <td style="padding:40px;width:35%;vertical-align:middle;padding-top:80px">
				<div class="one">
				  <img src='images/flow.png' width="250">
				</div>
			  </td>
			  <td style="padding:20px;width:75%;vertical-align:middle">
				<a href="https://aclanthology.org/2021.acl-long.355/">
				  <papertitle>Continuous language generative flow</papertitle>
				</a>
				<br>
				<strong>Zineng Tang</strong>,
				<a href="https://www.cs.unc.edu/~shiyue/">Shiyue Zhang</a>,
				<a href="https://hyounghk.github.io/">Hyounghun Kim</a>,
			    <a href="https://www.cs.unc.edu/~mbansal/">Mohit Bansal</a>
				<br>
				<em>ACL</em>, 2021
				<br>
				<a href="https://github.com/zinengtang/ContinuousFlowNLG">github</a>
				/
				<a href="https://aclanthology.org/2021.acl-long.355/">paper</a>
				<p></p>
				<p>
				We built a language framework basde on continuous generative flow.
				</p>
			  </td>
			</tr>	
            <tr onmouseout="stop()" onmouseover="start()">
			  <td style="padding:40px;width:35%;vertical-align:middle;padding-top:80px">
				<div class="one">
				  <img src='images/densecap.png' width="250">
				</div>
			  </td>
			  <td style="padding:20px;width:75%;vertical-align:middle">
				<a href="https://arxiv.org/pdf/2005.06409">
				  <papertitle>Dense-caption matching and frame-selection gating for temporal localization in VideoQA</papertitle>
				</a>
				<br>
				<a href="https://hyounghk.github.io/">Hyounghun Kim</a>,
				<strong>Zineng Tang</strong>,
			    <a href="https://www.cs.unc.edu/~mbansal/">Mohit Bansal</a>
				<br>
				<em>ACL</em>, 2020
				<br>
				<a href="https://github.com/zinengtang/ContinuousFlowNLG">github</a>
				/
				<a href="https://arxiv.org/pdf/2005.06409">arxiv</a>
				<p></p>
				<p>
				We built a video QA framework basde on various techniques like dense captions.
				</p>
			  </td>
			</tr>
        </tbody></table>
		
		<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
		  <tr>
			<td>
			  <heading>Education</heading>
			</td>
		  </tr>
		</tbody></table>
		<table width="100%" align="center" border="0" cellpadding="20"><tbody>  
			<tr>
				<td style="padding:20px;width:25%;vertical-align:middle"><img width="100" src="images/unc.png"></td>
				<td width="75%" valign="center">
				  <p>UNC Chapel Hill, B.S. in Mathematics, 2019 - present </p>
				</td>
			</tr>	
		</tbody></table>
		
		<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
		  <tr>
			<td>
			  <heading>Experiences</heading>
			</td>
		  </tr>
		</tbody></table>
		<table width="100%" align="center" border="0" cellpadding="20"><tbody>  
			<tr>
				<td style="padding:20px;width:25%;vertical-align:middle"><img width="100" src="images/murgelab-logo.png"></td>
				<td width="75%" valign="center">
				  <p><a href="https://murgelab.cs.unc.edu/">MURGe-Lab</a>, Undergraduate Research Assistant </p>
				</td>
			</tr>	
			<tr>
				<td style="padding:20px;width:25%;vertical-align:middle"><img width="100" src="images/microsoft.png"></td>
				<td width="75%" valign="center">
				  <p><a href="https://www.microsoft.com/en-us/research/group/cognitive-services-research/">Microsoft Azure Cognitive Services Research</a>, Research Intern </p>
				</td>
			</tr>
		</tbody></table>
		
		<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
		  <tr>
			<td>
			  <heading>Awards</heading>
			</td>
		  </tr>
		</tbody></table>
		<table width="100%" align="center" border="0" cellpadding="20"><tbody>
							
			<tr>
				<td width="100%" valign="center">
				  <p>NeurIPS 2022 Scholar Award</p>
				  <p>Honorable Mention, Outstanding Undergraduate Researcher Award 2022. Computing Research Association (CRA) (2022)</p>
				</td>
			</tr>
		</tbody></table>	  
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Website template mainly borrowed from <a href="https://jonbarron.info/">Here</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
